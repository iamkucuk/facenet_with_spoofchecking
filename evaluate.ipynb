{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "import sys, os\n",
    "import glob\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from facenet_pytorch.models.mtcnn import MTCNN, prewhiten\n",
    "from facenet_pytorch.models.inception_resnet_v1 import InceptionResnetV1, get_torch_home\n",
    "from facenet_pytorch.models.utils.detect_face import extract_face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_image(path, trans):\n",
    "    img = Image.open(path)\n",
    "    img = trans(img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "trans = transforms.Compose([\n",
    "    transforms.Resize(512)\n",
    "])\n",
    "\n",
    "trans_cropped = transforms.Compose([\n",
    "    np.float32,\n",
    "    transforms.ToTensor(),\n",
    "    prewhiten\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset = datasets.ImageFolder('dataset/lfw', transform=trans)\n",
    "dataset.idx_to_class = {k: v for v, k in dataset.class_to_idx.items()}\n",
    "loader = DataLoader(dataset, collate_fn=lambda x: x[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "mtcnn = MTCNN(device=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4771a1e0d1984eb5ace9bac089460a37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=13233), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at CPUAllocator.cpp:56] posix_memalign(&data, gAlignment, nbytes) == 0. 12 vs 0\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-47e74f8205e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0maligned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_align\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0maligned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maligned\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at CPUAllocator.cpp:56] posix_memalign(&data, gAlignment, nbytes) == 0. 12 vs 0\n"
     ]
    }
   ],
   "source": [
    "total_item = len(dataset)\n",
    "names = []\n",
    "aligned = []\n",
    "for img, idx in tqdm(loader):\n",
    "    name = dataset.idx_to_class[idx]\n",
    "    # start = time()\n",
    "    img_align = mtcnn(img, save_path = \"data/aligned/{}/{}.png\".format(name, str(idx)))\n",
    "    # print('MTCNN time: {:6f} seconds'.format(time() - start))\n",
    "\n",
    "    if img_align is not None:\n",
    "        names.append(name)\n",
    "        aligned.append(img_align)\n",
    "\n",
    "aligned = torch.stack(aligned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "resnet = InceptionResnetV1(pretrained='casia-webface').eval().cpu()\n",
    "\n",
    "start = time()\n",
    "embs = resnet(aligned)\n",
    "print('\\nResnet time: {:6f} seconds\\n'.format(time() - start))\n",
    "\n",
    "dists = [[(emb - e).norm().item() for e in embs] for emb in embs]\n",
    "\n",
    "print('\\nOutput:')\n",
    "print(pd.DataFrame(dists, columns=names, index=names))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
